{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36ms3fs\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcore\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m S3FileSystem\n",
      "s3 = S3FileSystem()\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m########\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdistributed\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdataparallel\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36msdp\u001b[39;49;00m\n",
      "sdp.init()\n",
      "\u001b[37m########\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m########\u001b[39;49;00m\n",
      "gpus = tf.config.experimental.list_physical_devices(\u001b[33m'\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[34mfor\u001b[39;49;00m gpu \u001b[35min\u001b[39;49;00m gpus:\n",
      "    tf.config.experimental.set_memory_growth(gpu, \u001b[34mTrue\u001b[39;49;00m)\n",
      "\u001b[34mif\u001b[39;49;00m gpus:\n",
      "    tf.config.experimental.set_visible_devices(gpus[sdp.local_rank()], \u001b[33m'\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[37m########\u001b[39;49;00m\n",
      "\n",
      "bucket = \u001b[33m'\u001b[39;49;00m\u001b[33mzhixue.sagemaker.iad\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "train_images = np.load(s3.open(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mlego-simple-train-images.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "train_labels = np.load(s3.open(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mlego-simple-train-labels.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "test_images = np.load(s3.open(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mlego-simple-test-images.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "test_labels = np.load(s3.open(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mlego-simple-test-labels.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "\n",
      "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).repeat(\u001b[34m200\u001b[39;49;00m).shuffle(\u001b[34m10000\u001b[39;49;00m).batch(\u001b[34m128\u001b[39;49;00m)\n",
      "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(\u001b[34m128\u001b[39;49;00m)\n",
      "    \n",
      "class_names = [\u001b[33m'\u001b[39;49;00m\u001b[33m2x3 Brick\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m2x2 Brick\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1x3 Brick\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m2x1 Brick\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1x1 Brick\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "               \u001b[33m'\u001b[39;49;00m\u001b[33m2x2 Macaroni\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m2x2 Curved End\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mCog 16 Tooth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1x2 Handles\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1x2 Grill\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "train_loss = tf.keras.metrics.Mean(name=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "test_loss = tf.keras.metrics.Mean(name=\u001b[33m'\u001b[39;49;00m\u001b[33mtest_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u001b[33m'\u001b[39;49;00m\u001b[33mtest_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "model = keras.Sequential([\n",
      "    keras.layers.Flatten(input_shape=(\u001b[34m48\u001b[39;49;00m, \u001b[34m48\u001b[39;49;00m)),\n",
      "    keras.layers.Dense(\u001b[34m128\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "    keras.layers.Dense(\u001b[34m10\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "])\n",
      "\n",
      "\u001b[37m########\u001b[39;49;00m\n",
      "optimizer = tf.keras.optimizers.Adam()\n",
      "\u001b[37m#optimizer = tf.optimizers.Adam(0.000125 * sdp.size())\u001b[39;49;00m\n",
      "checkpoint_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
      "\u001b[37m########\u001b[39;49;00m\n",
      "\n",
      "\u001b[90m@tf\u001b[39;49;00m.function\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_step\u001b[39;49;00m(images, labels, first_batch):\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.GradientTape() \u001b[34mas\u001b[39;49;00m tape:\n",
      "    \n",
      "        \u001b[37m# training=True is only needed if there are layers with different\u001b[39;49;00m\n",
      "        \u001b[37m# behavior during training versus inference (e.g. Dropout).\u001b[39;49;00m\n",
      "        predictions = model(images, training=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        loss = loss_object(labels, predictions)\n",
      "        \n",
      "    \u001b[37m########\u001b[39;49;00m\n",
      "    \u001b[37m# Wrap tf.GradientTape with the library's DistributedGradientTape\u001b[39;49;00m\n",
      "    tape = sdp.DistributedGradientTape(tape)\n",
      "    \u001b[37m########\u001b[39;49;00m\n",
      "    \n",
      "    gradients = tape.gradient(loss, model.trainable_variables)\n",
      "    optimizer.apply_gradients(\u001b[36mzip\u001b[39;49;00m(gradients, model.trainable_variables))\n",
      "    \n",
      "    \u001b[37m########\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m first_batch:\n",
      "        \u001b[37m# SageMaker data parallel: Broadcast model and optimizer variables\u001b[39;49;00m\n",
      "        sdp.broadcast_variables(model.variables, root_rank=\u001b[34m0\u001b[39;49;00m)\n",
      "        sdp.broadcast_variables(optimizer.variables(), root_rank=\u001b[34m0\u001b[39;49;00m)\n",
      "    \u001b[37m########\u001b[39;49;00m\n",
      "    \n",
      "    train_loss(loss)\n",
      "    train_accuracy(labels, predictions)\n",
      "    \n",
      "\u001b[90m@tf\u001b[39;49;00m.function\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_step\u001b[39;49;00m(images, labels):\n",
      "    \u001b[37m# training=False is only needed if there are layers with different\u001b[39;49;00m\n",
      "    \u001b[37m# behavior during training versus inference (e.g. Dropout).\u001b[39;49;00m\n",
      "    predictions = model(images, training=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    t_loss = loss_object(labels, predictions)\n",
      "\n",
      "    test_loss(t_loss)\n",
      "    test_accuracy(labels, predictions)\n",
      "    \n",
      "EPOCHS = \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(EPOCHS):\n",
      "    \u001b[37m# Reset the metrics at the start of the next epoch\u001b[39;49;00m\n",
      "    train_loss.reset_states()\n",
      "    train_accuracy.reset_states()\n",
      "    test_loss.reset_states()\n",
      "    test_accuracy.reset_states()\n",
      "\n",
      "    batch = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m images, labels \u001b[35min\u001b[39;49;00m train_ds:\n",
      "        train_step(images, labels, batch == \u001b[34m0\u001b[39;49;00m)\n",
      "        \u001b[34mif\u001b[39;49;00m batch % \u001b[34m100\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mBatch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mLoss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_loss.result()\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mAccuracy: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_accuracy.result() * \u001b[34m100\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "            )\n",
      "        batch = batch + \u001b[34m1\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m t_images, t_labels \u001b[35min\u001b[39;49;00m test_ds:\n",
      "        test_step(t_images, t_labels)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTraining Loss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_loss.result()\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTraining Accuracy: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_accuracy.result() * \u001b[34m100\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTest Loss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_loss.result()\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTest Accuracy: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_accuracy.result() * \u001b[34m100\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "\u001b[37m######## \u001b[39;49;00m\n",
      "\u001b[37m# SMDataParallel: Save checkpoints only from master node.\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m sdp.rank() == \u001b[34m0\u001b[39;49;00m:\n",
      "    model.save(os.path.join(checkpoint_dir, \u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\u001b[37m######## \u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/TrainingTensorFlowImageClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(\n",
    "                        base_job_name='DistributedTrainingTensorFlowImageClassifier',\n",
    "                        source_dir='code',\n",
    "                        entry_point='TrainingTensorFlowImageClassifier.py',\n",
    "                        role=role,\n",
    "                        py_version='py37',\n",
    "                        framework_version='2.4.1',\n",
    "                        # For training with multinode distributed training, set this count. Example: 2\n",
    "                        instance_count=2,\n",
    "                        # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "                        instance_type= 'ml.p3.16xlarge',\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "                        # Training using SMDataParallel Distributed Training Framework\n",
    "                        distribution={'smdistributed':{\n",
    "                                            'dataparallel':{\n",
    "                                                    'enabled': True\n",
    "                                             }\n",
    "                                      }}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-24 10:04:02 Starting - Starting the training job...\n",
      "2021-04-24 10:04:25 Starting - Launching requested ML instancesProfilerReport-1619258641: InProgress\n",
      ".........\n",
      "2021-04-24 10:05:47 Starting - Preparing the instances for training....."
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "print(\"Storing {} as model_data\".format(model_data))\n",
    "%store model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
